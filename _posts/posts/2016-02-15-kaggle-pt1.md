---
layout: post
title: Example Kaggle Solution, Pt. 1 (Data pipelines)
date:   2016-02-16 23:28:13 -0600
categories: kaggle case-interview
---


Alright, so you're an aspiring data scientist, say a graduate student in a STEM field trying to get into private industry, and congratulations, you've made it to the case study round! What do I mean by *case study*? Ah, I mean a timed test with a *training set* and a *test set*. You've been instructed to build a model that will give predicted values for the observations contained in the test set, and then your (hopefully) future employer will compare your values with the test set's true values. Few things get more meritocratic than that!

<br />

## General steps involved:
1. Load your data. Poke around.
2. Decide on an appropriate mathematical/machine learning model you'd like to use.
3. Create a data processing pipeline for both your training and test data sets.
⋅⋅a. Pick features you'd like to try in your model. A **feature** is generally a column within your dataset fed into your machine learning model.
⋅⋅b. If applicable, standardize or normalize various features.
4. Tune the model's hyperparameters through validation.
5. Fit model with decided upon hyperparameters with all of your training data.
6. Get predictions on the processed test data.

<br />

## Example case interview: the Kaggle photo album classification project

[Kaggle][kaggle-link] is a data science competition website where individuals (and teams) compete, sometimes for big bucks, on data science projects hosted by big name companies (Deloitte and NASA, just to name a few) who want/need models and solutions to said data science projects. The projects run the gamut in difficulty, and I've picked an easier one that serves as a good example for what you might expect for a data science case interview: the [photo album quality prediction][pqp-link] project. You can track all of my relevant code and data [here][git-repo] in this Github repo.

**THE GIST:** Like I said, we'll need to process the training data, seek out new features, pick a model, train and tune it, and get our predictions. I'm going to do this using R, and since there's been a lot of buzz about __extreme gradient boosting__ over the last year or two, and the [xgboost library][xgboost] that implements the technique has won at least one Kaggle competition (probably more, I didn't feel like researching it further), we'll use xgboost to build a model/get predictions.

I'm not going to get into the nitty gritty of boosted trees and gradient boosting, but from a high level, you can think of it as a weighted sum of small decision trees. The size of the weighted sum (i.e. the number of decision trees) is a hyper-parameter that we'll tune. Each additional decision tree itself comes from fitting the residuals between the outcome and the existing weighted sum of small residual trees. Xgboost by default builds this type of boosted tree model for classification purposes, but it can also be used for continuous, regression learning purposes.

##Load the data, adding new features





[kaggle-link]: https://www.kaggle.com/
[pqp-link]: https://www.kaggle.com/c/PhotoQualityPrediction
[git-repo]: https://github.com/fineiskid/photo_kaggle
[xgboost]: https://www.youtube.com/watch?v=Og7CGAfSr_Y